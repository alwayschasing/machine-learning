#Descision Tree#
##实质##
从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树可能有多个，也可能一个没有。
我们需要一个与训练数据集矛盾较小的决策树，同时具有很好的泛化能力。
决策树学习用损失函数，通常为正则化的极大似然函数。
决策树学习包括：特征选择，决策树的生成，决策树的剪枝。
##信息增益##
特征：A ，训练数据集：D 
决策树学习目标为最大化信息增益g(D,A)=H(d)-H(D|A),对训练数据集D计算其每个特征的信息增益，选择信息增益最大的特征。
设训练数据集为D，有K类，特征A有n个不同的取值：
$$H(D)=-sum_{k=1}^K{\frac{\|C_k\|}{\|D\|}}\log_2\frac{\|C_k\|}{\|D\|}$$